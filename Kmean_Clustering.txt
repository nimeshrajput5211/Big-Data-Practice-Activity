#3. Login to PySpark shell using pyspark
sc.setLogLevel("Error")

#4. Create RDD for the Historical Data available in HDFS
uber_data = sc.textFile('<Your_URL>')

#5. Verify the first record from the above RDD 
uber_data.first()

#6. Create a new RDD with the name uber_fea_data with each field separated by "," 
# Use the transformation "map" and apply a lambda function split on ","
uber_fea_data = uber_data.map(lambda line : line.split(','))

#7. Verify first record
uber_fea_data.first()

#8. Verify first few records
uber_fea_data.take(2)

#9. Verify the type of RDD
type(uber_fea_data)

#10. Take a sample of 2% from the above data and write this sample as CSV file into Local file system or HDFS
#Take a sample of 2% from the above data into RDD sampleUberData
sampleUberData = uber_fea_data.sample(False, .02, 12345)

#11. Write a function (toCSVLine) which parses above RDD as CSV data.
def toCSVLine(data):
    return ','.join(str(d) for d in data)

#12. Create a new RDD lines, by applying a map transformation and the function toCSVLine on each line.
lines = sampleUberData.map(toCSVLine)

#13. Write the above RDD lines as text file
lines.coalesce(1).saveAsTextFile('<System_URL>')


Step 2: Build a K-Means cluster algorithm and experiments with different K values and compute the WSSSE values and 
derive a optimal K-Value.

1. Transfer/Copy the above sample file to Local file system (Windows/Mac)

2. Execute the code from 0_uberSample.py file.

===================================================================================================================
Code Snippet from 0_uberSample.py file

## necessary imports
import pandas as pd
import numpy as np
import scipy as sp
from sklearn.cluster import KMeans
get_ipython().magic(u'matplotlib inline')
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

## Create a dataframe for the CSV data 
sampleData = pd.read_csv("<CSV Data Path>/part-00000",  names = ['date', 'longitude', 'latitude', 'baseLLC'])

## Verify above dataframe
sampleData.head()

## Extract only longitude and latitude from the data
columns = ['longitude', 'latitude']
featuresData = pd.DataFrame(sampleData, columns=columns)

## Verify above dataframe
featuresData.head()

## Verify features datatype
featuresData['longitude'].dtype
featuresData['latitude'].dtype

### For the purposes of this example, we store feature data from our
### dataframe `featuresData`, in the `f1` and `f2` arrays. We combine this into
### a feature matrix `X` before entering it into the algorithm.
f1 = featuresData['longitude'].values
f2 = featuresData['latitude'].values
X=np.matrix(zip(f1,f2))

## Display X
## Draw a scatter plot with above features
plt.scatter(f1,f2)
plt.show()

## Build K Means with the 2 clusters
kmeans = KMeans(n_clusters=2).fit(X)

## Derive Centroids
centroids = kmeans.cluster_centers_

## Derive Labels
labels = kmeans.labels_

Part 3: Using the above value train/build and save the model on the data available in HDFS

1. Import necessary packages
from pyspark.mllib.clustering import KMeans, KMeansModel
from numpy import array
from math import sqrt

2. Write a function (convertDataFloat) to convert the two features longitude and latitude into float type
def convertDataFloat(line):
    return array([float(line[1]),float(line[2])])

3. Apply Map transformation on the uber_fea_data RDD and apply convertDataFloat function on each line and create parsedUberData RDD
uber_fea_data = uber_data.map(lambda data:data.split(','))
parsedUberData = uber_fea_data.map(lambda line : convertDataFloat(line)) 

4. Verify first line from the above parsed RDD, parsedUberData
parsedUberData.first()

5. Verify the total count in the above RDD, parsedUberData
parsedUberData.count()

6. Train the model on cluster data with K = 8, 
clusters = KMeans.train(parsedUberData,8, maxIterations=10,runs=10, initializationMode="random")

7. Verify cluster centers
clusters.centers

8. Function to compute WSSSE 
def wsssError(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

9. Compute and display Total WSSSE
WSSSE = parsedUberData.map(lambda point: wsssError(point)).reduce(lambda x, y: x + y)
print("Within Set Sum of Squared Error = " + str(WSSSE))

10. Save the above model
clusters.save(sc  , "/user/<user-id>/uber/kmeanModel")

## Print Centroids
print(centroids)

## Print Labels
print(labels)

## Experiment/Build K means for different K values, from 1 to 20
K = range(1,20)
KM = [KMeans(n_clusters=k).fit(X) for k in K]
centroids = [k.cluster_centers_ for k in KM]

## Find with in sum of squared error
from scipy.spatial.distance import cdist, pdist

D_k = [cdist(X[:10000], cent, 'euclidean') for cent in centroids]
cIdx = [np.argmin(D,axis=1) for D in D_k]
dist = [np.min(D,axis=1) for D in D_k]
sumWithinSS = [sum(d) for d in dist]

## Total with-in sum of squared error
wcss = [sum(d**2) for d in dist]
tss = sum(pdist(X[:10000])**2)/X.shape[0]
bss = tss-wcss


## Elbow curve
#1 
kIdx = 8

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(K, sumWithinSS, 'b*-')
ax.plot(K[kIdx], sumWithinSS[kIdx], marker='o', markersize=12, 
markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')
plt.grid(True)
plt.xlabel('Number of clusters')
plt.ylabel('within-cluster sum of squares')
plt.title('Elbow for KMeans clustering')



#2
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(K, bss/tss*100, 'b*-')
plt.grid(True)
plt.xlabel('Number of clusters')
plt.ylabel('Percentage of variance explained')
plt.title('Elbow for KMeans clustering')


